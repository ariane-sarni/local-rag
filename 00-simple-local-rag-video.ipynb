{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b5c126-d7e2-4ec7-b1a5-82a9534fc686",
   "metadata": {},
   "source": [
    "# Create and run a RAG model pipeline from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42d426-312f-4532-9a1b-f6fe9b07d2d6",
   "metadata": {},
   "source": [
    "## What does RAG stand for?\n",
    "\n",
    "Stands for Retrieval Augmented Generation.\n",
    "\n",
    "Essentially takes info and passes it to LLM, then LLM generates output based on that information.\n",
    "\n",
    "* Retrieval - Find info given a certain question, in this example with a PDF discussing nutrition, \"What are micronutrients, and what do they do?\". The LLM retrieves text related to micronutrients from the textbook.  \n",
    "* Augmented - Take relevant info and augment are input (The prompt we are giving) to an LLM with that info.\n",
    "* Generation - Takes first two steps, passes to an LLM for generative outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57026e3f-6e76-47de-8780-f6897edded76",
   "metadata": {},
   "source": [
    "## What is the usage of Retrieval Augmented Generation?\n",
    "\n",
    "RAG improves generation output of LLM.\n",
    "\n",
    "1. Prevents hallucinations - LLMs are given information factually - they're less likely to hallucinate when they are given actual info as opposed to making up their own information.\n",
    "2.  Allows LLM to work with custom data - Since LLMs are already trained with internet scaled data, they have a decent understanding of language in general but because of this their responses can be fairly generic. RAG essentially helps create specific responses based on input documents (i.e. your own companies customer support docs, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf83a1-0dc3-4872-b71b-8497d99d7880",
   "metadata": {},
   "source": [
    "## How can RAG be used?\n",
    "\n",
    "* Customer support chat - Have an existing LLM supported by documentation from the respective company. Retrieves documents already created on how to do certain things, have the LLM use that data when answering. Essentially chatbot for given documentation.\n",
    "* Email chain analysis - If you're a company with a lost of emails of customer chains, could use a RAG by feeding all of these emails into an LLM, then using said LLM to process that info into more structured data for you to parse through. Maybe turn to JSON.\n",
    "* Company internal documentation chat\n",
    "* Textbook Q&A\n",
    "\n",
    "Essentially: Take relevent documents to a query, and process with an LLM.\n",
    "\n",
    "Could think of LLM as a calculator for words in this instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0f5a2-e86a-4d8e-8113-65b1c168892c",
   "metadata": {},
   "source": [
    "## Why run locally?\n",
    "\n",
    "1. No API calls - Potentially faster speed since you're not calling some other LLM.\n",
    "2. Privacy - Perhaps you're using internal documents you don't want to feed somewhere.\n",
    "3. Cost - No pricing for API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02c335-3f9c-4aeb-8a50-ef3d8baf705f",
   "metadata": {},
   "source": [
    "## Build goals: \n",
    "\n",
    "Build RAG pipeline that runs locally on my device. It will do the following:\n",
    "\n",
    "1. Open up a document I pass it, whether it be a PDF, .MD file, etc.\n",
    "2. Format the text for an embedding model.\n",
    "3. Embed all the neccesary chunks of text and turn it into a numerical representation which can be stored.\n",
    "4. Build a retrieval system (vector search?) to find relevant chunks of text based on the query.\n",
    "5. Generate a prompt that incorporates the retrieved pieces of text.\n",
    "6. Generate an answer to the query based on the passage of the document with an LLM.\n",
    "\n",
    "1. Steps 1-3: Document preprocessing and embedding creation.\n",
    "2. Step 4-6: Search and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76708b-0a23-45e2-8dbf-6adae0928a58",
   "metadata": {},
   "source": [
    "### 1. Document/Text processing and embedding creation\n",
    "\n",
    "Neccesary:\n",
    "* PDF document of choice (Not neccesarily PDF - could be Markdown, .txt, etc.)\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import document.\n",
    "2. Proces text for embedding (split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use. (Will store on file for many years, however long you need.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b37eac-788e-4db7-84e2-a6861db60826",
   "metadata": {},
   "source": [
    "### Import PDF document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969e07f6-fab5-4548-9755-40f1b63a8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File doesn't exist, downloading...\n",
      "[INFO] The file has been downloaded and saved as human-nutrition-text.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Grab PDF path\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"[INFO] File doesn't exist, downloading...\")\n",
    "\n",
    "    # Enter URL of the PDF\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # Local filename to save the file we just downloaded.\n",
    "\n",
    "    filename = pdf_path\n",
    "\n",
    "    # Send a GET request to the URL. \n",
    "    response = requests.get(url)\n",
    "\n",
    "    # See if request was successful \n",
    "    if response.status_code == 200:\n",
    "        # Open file and save it.\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {pdf_path} already exists.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafc7c0-5070-4261-94b9-cb960cf30d72",
   "metadata": {},
   "source": [
    "We now have the PDF! So we can open it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0b0f73-907b-487f-892c-8224fdd7b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c487473a6448c3b8ed68a7ab59233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pages_and_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     18\u001b[39m         pages_and_text.append({\u001b[33m\"\u001b[39m\u001b[33mpage_number\u001b[39m\u001b[33m\"\u001b[39m: page_number - \u001b[32m41\u001b[39m,\n\u001b[32m     19\u001b[39m                                \u001b[33m\"\u001b[39m\u001b[33mpage_char_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text), \n\u001b[32m     20\u001b[39m                                \u001b[33m\"\u001b[39m\u001b[33mpage_word_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text.split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     21\u001b[39m                                \u001b[33m\"\u001b[39m\u001b[33mpage_sentence_count_raw\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text.split(\u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     22\u001b[39m                                \u001b[33m\"\u001b[39m\u001b[33mpage_token_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text) / \u001b[32m4\u001b[39m, \u001b[38;5;66;03m# 1 token = ~4 characters.\u001b[39;00m\n\u001b[32m     23\u001b[39m                                \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text})\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pages_and_texts\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m pages_and_texts = \u001b[43mopen_and_read_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m pages_and_texts[:\u001b[32m2\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mopen_and_read_pdf\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m     16\u001b[39m text = page.get_text()\n\u001b[32m     17\u001b[39m text = text_formatter(text=text)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mpages_and_text\u001b[49m.append({\u001b[33m\"\u001b[39m\u001b[33mpage_number\u001b[39m\u001b[33m\"\u001b[39m: page_number - \u001b[32m41\u001b[39m,\n\u001b[32m     19\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mpage_char_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text), \n\u001b[32m     20\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mpage_word_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text.split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     21\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mpage_sentence_count_raw\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text.split(\u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m     22\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mpage_token_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(text) / \u001b[32m4\u001b[39m, \u001b[38;5;66;03m# 1 token = ~4 characters.\u001b[39;00m\n\u001b[32m     23\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text})\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pages_and_texts\n",
      "\u001b[31mNameError\u001b[39m: name 'pages_and_text' is not defined"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Potentially more text formatting functions if you need them can go here.\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_text.append({\"page_number\": page_number - 41,\n",
    "                               \"page_char_count\": len(text), \n",
    "                               \"page_word_count\": len(text.split(\" \")),\n",
    "                               \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                               \"page_token_count\": len(text) / 4, # 1 token = ~4 characters.\n",
    "                               \"text\": text})\n",
    "        return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71c7b0-92bd-4b85-9d71-c82fed1a9a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
