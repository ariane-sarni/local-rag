{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b5c126-d7e2-4ec7-b1a5-82a9534fc686",
   "metadata": {},
   "source": [
    "# Create and run a RAG model pipeline from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42d426-312f-4532-9a1b-f6fe9b07d2d6",
   "metadata": {},
   "source": [
    "## What does RAG stand for?\n",
    "\n",
    "Stands for Retrieval Augmented Generation.\n",
    "\n",
    "Essentially takes info and passes it to LLM, then LLM generates output based on that information.\n",
    "\n",
    "* Retrieval - Find info given a certain question, in this example with a PDF discussing nutrition, \"What are micronutrients, and what do they do?\". The LLM retrieves text related to micronutrients from the textbook.  \n",
    "* Augmented - Take relevant info and augment are input (The prompt we are giving) to an LLM with that info.\n",
    "* Generation - Takes first two steps, passes to an LLM for generative outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57026e3f-6e76-47de-8780-f6897edded76",
   "metadata": {},
   "source": [
    "## What is the usage of Retrieval Augmented Generation?\n",
    "\n",
    "RAG improves generation output of LLM.\n",
    "\n",
    "1. Prevents hallucinations - LLMs are given information factually - they're less likely to hallucinate when they are given actual info as opposed to making up their own information.\n",
    "2.  Allows LLM to work with custom data - Since LLMs are already trained with internet scaled data, they have a decent understanding of language in general but because of this their responses can be fairly generic. RAG essentially helps create specific responses based on input documents (i.e. your own companies customer support docs, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf83a1-0dc3-4872-b71b-8497d99d7880",
   "metadata": {},
   "source": [
    "## How can RAG be used?\n",
    "\n",
    "* Customer support chat - Have an existing LLM supported by documentation from the respective company. Retrieves documents already created on how to do certain things, have the LLM use that data when answering. Essentially chatbot for given documentation.\n",
    "* Email chain analysis - If you're a company with a lost of emails of customer chains, could use a RAG by feeding all of these emails into an LLM, then using said LLM to process that info into more structured data for you to parse through. Maybe turn to JSON.\n",
    "* Company internal documentation chat\n",
    "* Textbook Q&A\n",
    "\n",
    "Essentially: Take relevent documents to a query, and process with an LLM.\n",
    "\n",
    "Could think of LLM as a calculator for words in this instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0f5a2-e86a-4d8e-8113-65b1c168892c",
   "metadata": {},
   "source": [
    "## Why run locally?\n",
    "\n",
    "1. No API calls - Potentially faster speed since you're not calling some other LLM.\n",
    "2. Privacy - Perhaps you're using internal documents you don't want to feed somewhere.\n",
    "3. Cost - No pricing for API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02c335-3f9c-4aeb-8a50-ef3d8baf705f",
   "metadata": {},
   "source": [
    "## Build goals: \n",
    "\n",
    "Build RAG pipeline that runs locally on my device. It will do the following:\n",
    "\n",
    "1. Open up a document I pass it, whether it be a PDF, .MD file, etc.\n",
    "2. Format the text for an embedding model.\n",
    "3. Embed all the neccesary chunks of text and turn it into a numerical representation which can be stored.\n",
    "4. Build a retrieval system (vector search?) to find relevant chunks of text based on the query.\n",
    "5. Generate a prompt that incorporates the retrieved pieces of text.\n",
    "6. Generate an answer to the query based on the passage of the document with an LLM.\n",
    "\n",
    "1. Steps 1-3: Document preprocessing and embedding creation.\n",
    "2. Step 4-6: Search and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76708b-0a23-45e2-8dbf-6adae0928a58",
   "metadata": {},
   "source": [
    "### 1. Document/Text processing and embedding creation\n",
    "\n",
    "Neccesary:\n",
    "* PDF document of choice (Not neccesarily PDF - could be Markdown, .txt, etc.)\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "1. Import document.\n",
    "2. Proces text for embedding (split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use. (Will store on file for many years, however long you need.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b37eac-788e-4db7-84e2-a6861db60826",
   "metadata": {},
   "source": [
    "### Import PDF document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969e07f6-fab5-4548-9755-40f1b63a8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File doesn't exist, downloading...\n",
      "[INFO] The file has been downloaded and saved as human-nutrition-text.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Grab PDF path\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"[INFO] File doesn't exist, downloading...\")\n",
    "\n",
    "    # Enter URL of the PDF\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # Local filename to save the file we just downloaded.\n",
    "\n",
    "    filename = pdf_path\n",
    "\n",
    "    # Send a GET request to the URL. \n",
    "    response = requests.get(url)\n",
    "\n",
    "    # See if request was successful \n",
    "    if response.status_code == 200:\n",
    "        # Open file and save it.\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File {pdf_path} already exists.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafc7c0-5070-4261-94b9-cb960cf30d72",
   "metadata": {},
   "source": [
    "We now have the PDF! So we can open it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf0b0f73-907b-487f-892c-8224fdd7b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233e8aff5da34946b67113db367042cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Potentially more text formatting functions if you need them can go here.\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,\n",
    "                               \"page_char_count\": len(text), \n",
    "                               \"page_word_count\": len(text.split(\" \")),\n",
    "                               \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                               \"page_token_count\": len(text) / 4, # 1 token = ~4 characters.\n",
    "                               \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71c11f52-ec8c-43b7-bc74-a72a5fe73c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1056,\n",
       "  'page_char_count': 1545,\n",
       "  'page_word_count': 269,\n",
       "  'page_sentence_count_raw': 16,\n",
       "  'page_token_count': 386.25,\n",
       "  'text': 'butter on your toast, making your own salad dressing using  olive oil, vinegar or lemon juice, and herbs, cooking with  olive oil exclusively, or simply adding a dose of it to your  favorite meal.11  The Raw Food Diet  The raw food diet is followed by those who avoid cooking as much  as possible in order to take advantage of the full nutrient content  of foods. The principle behind raw foodism is that plant foods in  their natural state are the most wholesome for the body. The raw  food diet is not a weight-loss plan, it is a lifestyle choice. People who  practice raw foodism eat only uncooked and unprocessed foods,  emphasizing whole fruits and vegetables. Staples of the raw food  diet include whole grains, beans, dried fruits, seeds and nuts,  seaweed, sprouts, and unprocessed produce. As a result, food  preparation mostly involves peeling, chopping, blending, straining,  and dehydrating fruits and vegetables.  The positive aspects of this eating method include consuming  foods that are high in fiber and nutrients, and low in calories and  saturated fat. However, the raw food diet offers little in the way of  protein, dairy, or fats, which can cause deficiencies of the vitamins  A, D, E, and K. In addition, not all foods are healthier uncooked,  such as spinach and tomatoes. Also, cooking eliminates potentially  11. More Olive Oil in Diet Could Cut Stroke Risk: Study.  MedicineNet.com. https://www.medicinenet.com/ script/main/art.asp?articlekey=145823. Published 2011.  Accessed April 15,2018.  1056  |  Comparing Diets'},\n",
       " {'page_number': 606,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': 418,\n",
       "  'page_char_count': 1717,\n",
       "  'page_word_count': 292,\n",
       "  'page_sentence_count_raw': 18,\n",
       "  'page_token_count': 429.25,\n",
       "  'text': 'sources of lysine. Following a vegetarian diet and getting the  recommended protein intake is also made a little more difficult  because the digestibility of plant-based protein sources is lower  than the digestibility of animal-based protein.  To begin planning a more plant-based diet, start by finding out  which types of food you want to eat and in what amounts you should  eat them to ensure that you get the protein you need. The Dietary  Guidelines Advisory Committee (DGAC) has analyzed how three  different, plant-based dietary patterns can meet the recommended  dietary guidelines for all nutrients.2  The diets are defined in the following manner:  • Plant-based. Fifty percent of protein is obtained from plant  foods.  • Lacto-ovo vegetarian. All animal products except eggs and  dairy are eliminated.  • Vegan. All animal products are eliminated.  These diets are analyzed and compared to the more common  dietary pattern of Americans, which is referred to as the USDA Base  Diet. Table 6.6 “Percentage of “Meat and Beans Group” Components  in the USDA Base Diet, and Three Vegetarian Variations” and Table  7.7 “Proportions of Milk Products and Calcium-Fortified Soy  Products in the Base USDA Patterns and Three Vegetarian  Variations” can be used to help determine what percentage of  certain foods to eat when following a different dietary pattern. The  percentages of foods in the different groups are the proportions  2. Jacobs DR, et al. (2009).  Food, Plant Food, and  Vegetarian Diets in the US Dietary Guidelines:  Conclusions of an Expert Panel. American Journal of  Clinical Nutrition, 89(5). http://ajcn.nutrition.org/ content/89/5/1549S.short.  418  |  Proteins, Diet, and Personal Choices'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d71c7b0-92bd-4b85-9d71-c82fed1a9a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "\n",
       "   page_token_count                           text  \n",
       "0              7.25  Human Nutrition: 2020 Edition  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b84912-2284-4cf9-b2c9-33d7bcceca1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-41.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count          1.0              1.0              1.0                      1.0   \n",
       "mean         -41.0             29.0              4.0                      1.0   \n",
       "std            NaN              NaN              NaN                      NaN   \n",
       "min          -41.0             29.0              4.0                      1.0   \n",
       "25%          -41.0             29.0              4.0                      1.0   \n",
       "50%          -41.0             29.0              4.0                      1.0   \n",
       "75%          -41.0             29.0              4.0                      1.0   \n",
       "max          -41.0             29.0              4.0                      1.0   \n",
       "\n",
       "       page_token_count  \n",
       "count              1.00  \n",
       "mean               7.25  \n",
       "std                 NaN  \n",
       "min                7.25  \n",
       "25%                7.25  \n",
       "50%                7.25  \n",
       "75%                7.25  \n",
       "max                7.25  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf32f9-92df-48bc-9f4f-794b6936a722",
   "metadata": {},
   "source": [
    "### Further Text processing (Split pages into certain amount of sentences)\n",
    "\n",
    "Two ways to do this:\n",
    "1. Can do this by splitting on `\". \"`.\n",
    "2. We can do this with a NLP library such as spaCy and nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "529325df-d21b-4a2f-8ea1-257150ac0480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This is a second sentence., Even a third sentence.]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Create an instance of English.\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance. \n",
    "doc = nlp(\"This is a sentence. This is a second sentence. Even a third sentence.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# Print the sentences.\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52383ae3-dc54-4e36-9d0a-9a57b00a2a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': -41,\n",
       " 'page_char_count': 29,\n",
       " 'page_word_count': 4,\n",
       " 'page_sentence_count_raw': 1,\n",
       " 'page_token_count': 7.25,\n",
       " 'text': 'Human Nutrition: 2020 Edition',\n",
       " 'sentences': ['Human Nutrition: 2020 Edition'],\n",
       " 'page_sentence_count_spacy': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55c5e228-b301-4f1c-ad3d-e8abf81248e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae8b673f0dc4b7d8e6f831762c1bd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all of our sentences are strings.\n",
    "    # Default is spaCy datatype.)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the sentences.\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b3bae2-212f-46ad-89cc-49e769056a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 163,\n",
       "  'page_char_count': 664,\n",
       "  'page_word_count': 140,\n",
       "  'page_sentence_count_raw': 3,\n",
       "  'page_token_count': 166.0,\n",
       "  'text': 'CO2 + H20 +  ATP); sources  of water loss:  Skin and  lungs  (insensible  water loss  0.9 L/day),  Urine 1.5 L/ day, Feces 0.1  L/day.  TOTAL  intake 2.2 L/ day +  Metabolic  Production  0.3 L/day –  Output  (0.9+1.5=0.1)  L/day = 0 ”  class=”wp-i mage-141  size-full”  width=”629″  height=”777″ > Daily Fluid  Loss and  Gain    Dietary Recommendations  The Food and Nutrition Board of the Institute of Medicine (IOM) has  set the Adequate Intake (AI) for water for adult males at 3.7 liters  (15.6 cups) and at 2.7 liters (11 cups) for adult females.1 These intakes  1. Institute of Medicine Panel on Dietary Reference Intakes  Regulation of Water Balance  |  163',\n",
       "  'sentences': ['CO2 + H20 +  ATP); sources  of water loss:  Skin and  lungs  (insensible  water loss  0.9 L/day),  Urine 1.5 L/ day, Feces 0.1  L/day.',\n",
       "   ' TOTAL  intake 2.2 L/ day +  Metabolic  Production  0.3 L/day –  Output  (0.9+1.5=0.1)  L/day = 0 ”  class=”wp-i mage-141  size-full”  width=”629″  height=”777″ > Daily Fluid  Loss and  Gain    Dietary Recommendations  The Food and Nutrition Board of the Institute of Medicine (IOM) has  set the Adequate Intake (AI) for water for adult males at 3.7 liters  (15.6 cups) and at 2.7 liters (11 cups) for adult females.1 These intakes  1.',\n",
       "   'Institute of Medicine Panel on Dietary Reference Intakes  Regulation of Water Balance  |  163'],\n",
       "  'page_sentence_count_spacy': 3}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1acc6e02-7227-4ade-ab09-ae4edbe49b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>199.50</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "      <td>10.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.83</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.10</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>216.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>272.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           199.50                    10.52   \n",
       "std         348.86           560.38            95.83                     6.55   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     5.00   \n",
       "50%         562.50          1231.50           216.00                    10.00   \n",
       "75%         864.25          1603.50           272.00                    15.00   \n",
       "max        1166.00          2308.00           430.00                    39.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count           1208.00                    1208.00  \n",
       "mean             287.00                      10.32  \n",
       "std              140.10                       6.30  \n",
       "min                0.00                       0.00  \n",
       "25%              190.50                       5.00  \n",
       "50%              307.88                      10.00  \n",
       "75%              400.88                      15.00  \n",
       "max              577.00                      28.00  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe6156-eca9-49c2-846d-5a67f8f92bbc",
   "metadata": {},
   "source": [
    "### Time to chunk our sentences together.\n",
    "\n",
    "Essentially want to split large pieces of text into smaller ones. It's referred to as chunking or text splitting.\n",
    "\n",
    "There is no 'correct' way to do this.\n",
    "\n",
    "To keep it simple, splitting into groups of 10 sentences. However, could try any other, like 5, or 7, etc.\n",
    "\n",
    "There's frameworks ffor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987395fb-9619-44f0-b8ac-0d086a330290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
